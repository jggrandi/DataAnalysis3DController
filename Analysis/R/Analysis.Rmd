---
title: "Collaborative 3DUI Data Analysis"
author: "Jer√¥nimo G. Grandi"
date: "August 22, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(psych)
library(corrgram)
library(Hmisc)
library(tidyr)
library(dunn.test)
library(ggplot2)
```

``` {r dir,echo=FALSE}
setwd("/home/jeronimo/Documents/DataAnalysis3DController/Analysis/R")
#setwd("C:/Users/jeronimo/Documents/GitHub/DataAnalysis3DController/Analysis/R")
```


# Data analyisis

This is a on going analysis of the data collected in the user experiment performed.... 

# Comparisons between groups

##Hypotesis

* Groups with more than one member perform the tasks with less errors.
* Groups with two and three members perform the tasks with less errors than with one and four.
* Groups with more than one member perform the tasks faster.
* Groups with two and three members perform the tasks faster than with one and four.
* Task one and two are easier than three and four.


## Data Summary

``` {r load}
#sourceDataGroups <- read.csv("errorTimeAndVarPerTeam.csv",header = TRUE,  sep=";", dec = ",")
#sourceDataGroups <- read.csv("errorTimeAndVarPerTeamOnlyTask3and4.csv",header = TRUE,  sep="\t")
sourceDataGroups <- read.csv("errorMedianAndTimePerTeamOnlyTask3and4.csv", header = FALSE,  sep=";")
sourceDataGroups

#sourceDataTasks <- read.csv("errorAndTimePerTask.csv",header = TRUE,  sep="\t")
sourceDataTasks <- read.csv("errorMedianAndTimePerTaskOnlyTask3and4.csv",header = TRUE,  sep="\t")
sourceDataTasks
```
   
Below the data is summarized:   

```{r summary}
describe(sourceDataGroups)
```
   
And ploted:

```{r boxplot1}
#boxplot(sourceDataGroups[1:4],xlab="Team members",ylab="Time",main="Time X Groups")
time <- gather(sourceDataGroups, "group", "time", 1:4)
ggplot(time, aes(x=group, y=time)) + geom_boxplot()+labs(title="Group size vs. Task completion time",x="Group Size", y = "Time (seconds)")+theme_bw()+ scale_x_discrete(breaks=c("Time.T1", "Time.T2", "Time.T3", "Time.T4"),labels=c("1", "2", "3","4"))

#boxplot(sourceDataGroups[5:8],xlab="Team members",ylab="Error",main="Error X Groups")
error <- gather(sourceDataGroups, "group", "error", 5:8)
ggplot(error, aes(x=group, y=error)) + geom_boxplot()+labs(title="Group size vs. Task accuracy",x="Group Size", y = "Accuracy")+theme_bw()+ scale_x_discrete(breaks=c("Error.T1", "Error.T2", "Error.T3", "Error.T4"),labels=c("1", "2", "3","4"))

variance <- gather(sourceDataGroups, "group", "variance", 9:12)
ggplot(variance, aes(x=group, y=variance)) + geom_boxplot()+labs(title="Group size vs. Work Distribution Balance",x="Group Size", y = "Work Distribution (variance")+theme_bw()+ scale_x_discrete(breaks=c("Var.T1", "Var.T2", "Var.T3", "Var.T4"),labels=c("1", "2", "3","4"))

```

### Correlation Analysis

The Pearson product-moment correlation coefficient is a measure of the linear correlation between two variables X and Y, giving a value between +1 and -1 inclusive, where 1 is total positive correlation, 0 is no correlation, and -1 is total negative correlation. It is widely used in the sciences as a measure of the degree of linear dependence between two variables. 

First we need to take the average of time and error of each team:

```{r corr}
#sourceDataTasks$MeanTime <- rowMeans(subset(sourceDataTasks, select = c(2,3,4,5)), na.rm = TRUE)
#sourceDataTasks$MeanError <- rowMeans(subset(sourceDataTasks, select = c(6,7,8,9)), na.rm = TRUE)
sourceDataTasks$MeanTime <- rowMeans(subset(sourceDataTasks, select = c(2,3)), na.rm = TRUE)
sourceDataTasks$MeanError <- rowMeans(subset(sourceDataTasks, select = c(4,5)), na.rm = TRUE)
```

Now we can make the pearson correlation analysis. It will tell us if there is a correlation between team members and time to solve the tasks, and team members and errors performed during the tasks.

First, the team members vs. time:

```{r corr2}
#teamTimeCorr <- sourceDataTasks[,c(1,10)]
teamTimeCorr <- sourceDataTasks[,c(1,7)]
teamTimeCorr <- rcorr(as.matrix(teamTimeCorr))
teamTimeCorr
```

Team members vs. error:

```{r corr3}
#teamErrorCorr <- sourceDataTasks[,c(1,11)]
teamErrorCorr <- sourceDataTasks[,c(1,8)]
teamErrorCorr <- rcorr(as.matrix(teamErrorCorr))
teamErrorCorr
```

As the  is greater than 0.05, we assume that there is no correlation between team members and time to complete the tasks. In the other hand, is < 0.0001, so there is a strong correlation between team members and errors performed. At this point we don't know if the errors grow with more team members or in the other way around.

### Shapiro-Wilk test
Before we conduce a variance test on the data to evaluate if more or less members cause more errors, we need to check if the data is normally distributed. For this test, we use the Shapiro-wilk test for each group of team members. 

```{r shapiro1}
sapply(lapply(sourceDataGroups[1:8], shapiro.test), `[`, c("statistic","p.value"))

```

As the P < 0.05 of most tests, we know that our data are not normally distributed. For this case, and because we are comparing distinct groups, the variance test will be performed by a Kruskal-Wallis. 

### Kruskal-Wallis



<!-- kruskal.test(sourceDataGroups[1:4]) -->
<!-- kruskal.test(sourceDataGroups[5:8]) -->


We perform the Kruskal test with Dunn posthoc with the time data:

```{r dunn1 posthoc}
library(dunn.test)
dunn.test(error$error,error$group, kw=TRUE, method="holm")
```

... and with the error data:

```{r dunn2 posthoc}
dunn.test(time$time,time$group, kw=TRUE, method="holm")
```

... and with the work distribution balance

```{r dunn3 posthoc}
dunn.test(variance$variance,variance$group, kw=TRUE, method="holm")
```

Excluding the first group size of the comparisons

```{r dunn3 posthoc}
variance2 <- gather(sourceDataGroups, "group", "variance2", 10:12)
dunn.test(variance2$variance2,variance2$group, kw=TRUE, method="holm")
```




# Comparison between tasks

## Hypotesis

* H1. Task 1 and 2 are easier than 3 and 4;
* H2. Task 4 is performed with less errors than task 3;
* H3. Task 4 is performed in less time than task 3;

If H2 or/and H3 is confirmed, we can say that the teams have improved their performance with only one trainning.


## Summary
The data is arranged by Task vs. Team members. Collumns are organized as follows:

* Members: Number of users in the team;
* T1, T2, T3, T4: Time to complete task 1 to 4;
* E1, E2, E3, E4: Errors in task 1 to 4.


Below the data is summarized:

```{r dataAverage}
 describe(sourceDataTasks[2:5])
 describe(sourceDataTasks[6:9])

```

### Plots

Time of task completion vs. Task for all combinations of teams:
The code used to generate the charts is:
```{r boxplot}
boxplot(sourceDataTasks[4:5],xlab="Tasks",ylab="Time",main="Time to complete the tasks")
boxplot(sourceDataTasks[8:9],xlab="Tasks",ylab="Error",main="Error perfored in the tasks")

```

## Analysis of time of completion per task

### Shapiro
First, we perform the Shapiro normality test. This test determine if the data is normally distributed. It is important to determine if the data is normally distributed to conduce posterior tests.

```{r shapiro 1}
sapply(lapply(sourceDataTasks[2:5], shapiro.test), `[`, c("statistic","p.value"))
```

As we can see, the $p-value$ of most Shapiro tests reveled that the data are not normally distributed. Since in this test the comparisons are made with the same subjects and we are varying the tasks, the next step is to perform a Friedman analysis.

### Analysis of learning between tasks 3 and 4 (Wilcoxon signed rank test)

<!-- ### Friedman -->

<!-- Friedman test is a non-parametric randomized block analysis of variance. Which is to say it is a non-parametric version of a one way ANOVA with repeated measures. That means that while a simple ANOVA test requires the assumptions of a normal distribution and equal variances (of the residuals), the Friedman test is free from those restriction. The price of this parametric freedom is the loss of power (of Friedman's test compared to the parametric ANOVa versions). -->

The hypotheses for the comparison across repeated measures are:

* H0: The distributions (whatever they are) are the same across repeated measures
* H1: The distributions across repeated measures are different

```{r wilcox}
wilcox.test(sourceDataTasks$Time.task.3,sourceDataTasks$Time.task.4,paired=TRUE)
wilcox.test(sourceDataTasks$Error.task.3,sourceDataTasks$Error.task.4,paired=TRUE)
```
<!-- The test statistic for the Friedman's test is a Chi-square with [(number of repeated measures)-1] degrees of freedom. A detailed explanation of the method for computing the Friedman test is available on Wikipedia. 

```{r fig.width=200,echo=FALSE}
library(png)
library(grid)
img <- readPNG("../../Results/friedman_TempoXTarefa.png")
grid.raster(img)
```

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/friedman_ErroXTarefa.png")
grid.raster(img)
```


## Analysis of learning between tasks 3 and 4

As the data is not normally distributed and the comparison is between tasks (instead of groups) we performed a Friedman analysis.



```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/fiedman_tasks3and4_error.png")
grid.raster(img)
```

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/fiedman_tasks3and4_time.png")
grid.raster(img)
```



## Analysis Correlation between Errors and Work division

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Correlacao_Error_DistribuicaoDeTrabalho.png")
grid.raster(img)
```

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Regressao_Error_DistTrabalho.png")
grid.raster(img)
```
-->

## Analysis of user role change 

### Correlation between Groups and user role change

```{r roles}
#sourceDataRoles <- read.csv("rolesPerTeam.csv",header = FALSE,  sep=";") #it is not working, but groupsRoles.csv works.. whyyy??? 
sourceDataRoles <- read.csv("groupsRoles.csv",header = FALSE,  sep="\t") 
sourceDataRoles
sourceGroupsErrorRoles <- read.csv("members_error_roles.csv",header = FALSE,  sep=";")
sourceGroupsErrorRoles


#rolesGroups <- gather(sourceDataRoles, "group", "roles", 1:4)
#rolesGroups <- rolesGroups[complete.cases(rolesGroups),]
#ggplot(rolesGroups, aes(x=group, y=roles)) + geom_boxplot()+labs(title="Group size vs. Team members roles",x="Group Size", y = "Roles")+theme_bw()+ scale_x_discrete(breaks=c("team1", "team2", "team3", "team4"),labels=c("1", "2", "3","4"))
#dunn.test(rolesGroups$roles,rolesGroups$group, kw = TRUE, method="holm")

ggplot(sourceDataRoles, aes(x=as.character(V1), y=V2)) + geom_boxplot()+labs(title="Group size vs. Team members roles",x="Group Size", y = "Roles")+theme_bw()
dunn.test(sourceDataRoles$V2,sourceDataRoles$V1, kw = TRUE, method="holm")


cor.test(sourceGroupsErrorRoles$V2,sourceGroupsErrorRoles$V3, alternative = "greater")
pairs(sourceGroupsErrorRoles)
lm_out <- lm(V2~V3, data=sourceGroupsErrorRoles)
plot(V2~V3, data=sourceGroupsErrorRoles)
abline(lm_out, col="red")
```
<!--
```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Correlation_Group_Roles.png")
grid.raster(img)
```

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Regressao_Group_Roles.png")
grid.raster(img)
```


### Variance Analysis (Kruskal) between Groups and user role change



```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Kruskal_Group_Roles.png")
grid.raster(img)
```

### Correlation between Groups, errors and roles

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Correlation_Group__Error_Roles.png")
grid.raster(img)
```

```{r fig.width=200,echo=FALSE}
img <- readPNG("../../Results/Regressao_Error_Roles.png")
grid.raster(img)
```

-->


<!-- # ### Kruskal-Wallis -->
<!-- #  -->
<!-- # A collection of data samples are independent if they come from unrelated populations and the samples do not affect each other. Using the Kruskal-Wallis Test, we can decide whether the population distributions are identical without assuming them to follow the normal distribution. -->
<!-- #  -->
<!-- #  -->
<!-- # ```{r fig.width=100,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/ErroXQntPessoas.png") -->
<!-- #  grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r fig.width=100,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/ErroPerCheckpointXGrupos.png") -->
<!-- #  grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r fig.width=100,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/kruskal_posHoc_Student.png") -->
<!-- #  grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # # Comparisons between tasks -->
<!-- #  -->
<!-- # ## Hypotesis -->
<!-- #  -->
<!-- # * Task 1 and 2 are easier than 3 and 4; -->
<!-- # * Task 4 is performed better than task 3 -->
<!-- #  -->
<!-- #  -->
<!-- # ## Summary -->
<!-- # First we set the environment, load and show the raw data.   -->
<!-- #   -->
<!-- # ``` {r raw} -->
<!-- # setwd("/home/jeronimo/Documents/DataAnalysis3DController/Analysis/R") -->
<!-- # dAvgPerTask <- read.csv("SummaryToR.csv") -->
<!-- # dAvgPerTask -->
<!-- # ``` -->
<!-- # The data is arranged by Task vs. Team members. Collumns are organized as follows:    -->
<!-- #  -->
<!-- # * Members: Number of users in the team;    -->
<!-- # * T1, T2, T3, T4: Time to complete task 1 to 4;     -->
<!-- # * E1, E2, E3, E4: Errors in task 1 to 4.  -->
<!-- #  -->
<!-- # We split the data for better manipulation: -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # dAvgTime <- subset(dAvgPerTask, select = c(2,3,4,5)) -->
<!-- # dAvgError <- subset(dAvgPerTask, select = c(6,7,8,9)) -->
<!-- # ``` -->
<!-- #  -->
<!-- # Below the data is summarized: -->
<!-- #  -->
<!-- # ```{r dataAverage} -->
<!-- # describe(dAvgTime) -->
<!-- # describe(dAvgError) -->
<!-- #  -->
<!-- # ``` -->
<!-- #  -->
<!-- # ### Plots -->
<!-- #  -->
<!-- # Time of task completion vs. Task for all combinations of teams: -->
<!-- # The code used to generate the charts is: -->
<!-- #  -->
<!-- # ```{r boxplot} -->
<!-- # boxplot(dAvgTime,xlab="Tasks",ylab="Time",main="Time to complete the tasks") -->
<!-- # boxplot(dAvgError,xlab="Tasks",ylab="Error",main="Error perfored in the tasks") -->
<!-- #  -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## Analysis of time of completion per task -->
<!-- #  -->
<!-- # ### Shapiro -->
<!-- # First, we perform the Shapiro normality test. This test determine if the data is normally distributed. It is important to determine if the data is normally distributed to conduce posterior tests. -->
<!-- #  -->
<!-- # ```{r shapiro 1} -->
<!-- # shap_dT <- lapply(dAvgTime, shapiro.test) -->
<!-- # shap_dE <- lapply(dAvgError, shapiro.test) -->
<!-- # res_shap_dT <- sapply(shap_dT, `[`, c("statistic","p.value")) -->
<!-- # res_shap_dE <- sapply(shap_dE, `[`, c("statistic","p.value")) -->
<!-- #  -->
<!-- # res_shap_dT -->
<!-- # res_shap_dE -->
<!-- # ``` -->
<!-- #  -->
<!-- # As we can see, the $p-value$ of most Shapiro tests reveled that the data are not normally distributed. Since in this test the comparisons are made with the same subjects and we are varying the tasks, the next step is to perform a Friedman analysis. -->
<!-- #  -->
<!-- # ### Friedman -->
<!-- #  -->
<!-- # Friedman test is a non-parametric randomized block analysis of variance. Which is to say it is a non-parametric version of a one way ANOVA with repeated measures. That means that while a simple ANOVA test requires the assumptions of a normal distribution and equal variances (of the residuals), the Friedman test is free from those restriction. The price of this parametric freedom is the loss of power (of Friedman's test compared to the parametric ANOVa versions). -->
<!-- #  -->
<!-- # The hypotheses for the comparison across repeated measures are:    -->
<!-- #  -->
<!-- # * H0: The distributions (whatever they are) are the same across repeated measures    -->
<!-- # * H1: The distributions across repeated measures are different    -->
<!-- #  -->
<!-- # The test statistic for the Friedman's test is a Chi-square with [(number of repeated measures)-1] degrees of freedom. A detailed explanation of the method for computing the Friedman test is available on Wikipedia. -->
<!-- #  -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/friedman_TempoXTarefa.png") -->
<!-- # grid.raster(img) -->
<!-- # ``` -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/friedman_ErroXTarefa.png")  -->
<!-- # grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## Analysis of learning between tasks 3 and 4 -->
<!-- #  -->
<!-- # As the data is not normally distributed and the comparison is between tasks (instead of groups) we performed a Friedman analysis. -->
<!-- #  -->
<!-- #  -->
<!-- #  -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/fiedman_tasks3and4_error.png")  -->
<!-- # grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/fiedman_tasks3and4_time.png")  -->
<!-- # grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## Analysis Correlation between Errors and Work division -->
<!-- #  -->
<!-- #  -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/Correlacao_Error_DistribuicaoDeTrabalho.png")  -->
<!-- # grid.raster(img) -->
<!-- # ``` -->
<!-- #  -->
<!-- #  -->
<!-- # ```{r fig.width=200,echo=FALSE} -->
<!-- # library(png) -->
<!-- # library(grid) -->
<!-- # img <- readPNG("../../Results/Regressao_Error_DistTrabalho.png")  -->
<!-- # grid.raster(img) -->
<!-- # ``` -->





