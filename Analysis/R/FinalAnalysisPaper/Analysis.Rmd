---
title: "Data Analysis of the Paper Design and Evaluation of a Handheld-based 3D User Interface for Collaborative Object Manipulation "
author: "Jer√¥nimo G. Grandi"
date: "September 26, 2016"
output:
  pdf_document: 
    number_sections: yes
    toc: yes
  html_document:
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(psych)
library(corrgram)
library(Hmisc)
library(tidyr)
library(dunn.test)
library(ggplot2)
```

``` {r dir,echo=FALSE}
setwd("/home/jeronimo/Documents/DataAnalysis3DController/Analysis/R/FinalAnalysisPaper/")
#setwd("C:/Users/jeronimo/Documents/GitHub/DataAnalysis3DController/Analysis/R")
```


# Data analyisis

This is the analysis of the data collected in the user experiment performed for the ACM CHI Conference on Human Factors in Computing Systems

# Design and Procedure
We aim to investigate the relationship between group sizes and the time and accuracy to complete the tasks. Furthermore, we intend to understand the influence of work distribution balance and work division in the performance of each group combination. Thus, the experiment follows a between subject design with Group size as the only independent variable, with one, two, three or four participants. Dependent variables collected were time to complete the task and accuracy of thegroup, and transformation actions (translation, rotation, scale or camera rotation), including duration and magnitude of the action performed by each individual subject. The accuracy is measured as described before in Section Collaborative 3D Manipulation Assessment.

##Task
We used the obstacle crossing game with three wall configurations. The training sessions consist of the first two walls.
The test session is formed by one trial for each practice wall and two trials for the tunnel. 

The results reported here only use the two trials in the tunnel task for the statistical analysis.

## Subjects
Sixty subjects participated voluntarily in this experiment (nine female), aged 24 years in average (SD=3.6). They were all Computer Science students with no movement restrictions on wrists and arms. Thirteen of the individuals had never used gestural interactions with Kinect, Wiimote or mobile devices. We arranged the participants in 5 groups of one, 7 groups of
two, 7 groups of three and 5 groups of four individuals.

## Hypotesis

* H1. Groups with more than one member complete the tasks faster  
* H2. Groups with more than one member complete the tasks with more accuracy  
* H3. For the tested group size range, if groups increase in members, the time to complete tasks drops proportionally  
* H4. For the tested group size range, if groups increase in members, the accuracy to complete tasks increase proportionally  

# Statistical Analysis Results


## Data Summary

``` {r load}
sourceDataGroups <- read.csv("errorMedianAndTimePerTeamOnlyTask3and4.csv", header = FALSE,  sep="\t")
sourceDataGroups

sourceDataTasks <- read.csv("errorMedianAndTimePerTaskOnlyTask3and4.csv",header = TRUE,  sep="\t")
sourceDataTasks
```

## Correlation Analysis

The Pearson product-moment correlation coefficient is a measure of the linear correlation between two variables X and Y, giving a value between +1 and -1 inclusive, where 1 is total positive correlation, 0 is no correlation, and -1 is total negative correlation. It is widely used in the sciences as a measure of the degree of linear dependence between two variables. 

First we need to take the average of time and error of each team:

```{r corr}
sourceDataTasks$MeanTime <- rowMeans(subset(sourceDataTasks, select = c(2,3)), na.rm = TRUE)
sourceDataTasks$MeanError <- rowMeans(subset(sourceDataTasks, select = c(4,5)), na.rm = TRUE)
```

Now we can make the pearson correlation analysis. It will tell us if there is a correlation between team members and time to solve the tasks, and team members and errors performed during the tasks.

The team members vs. time:

```{r corr2}
teamTimeCorr <- sourceDataTasks[,c(1,6)]
teamTimeCorr <- rcorr(as.matrix(teamTimeCorr))
teamTimeCorr
```

Team members vs. error:

```{r corr3}
teamErrorCorr <- sourceDataTasks[,c(1,7)]
teamErrorCorr <- rcorr(as.matrix(teamErrorCorr))
teamErrorCorr
```

As the time is greater than 0.05, we assume that there is no correlation between team members and time to complete the tasks. In the other hand, accuracy is < 0.0001, so there is a strong correlation between team members and errors performed. We have to conduct further analysis to understand the behavior of the accuracy increase. At this point we don't know if the errors grow with more team members or in the other way around.

## Shapiro-Wilk test
Before we conduce a variance test on the data to evaluate if more or less members cause more errors, we need to check if the data is normally distributed. For this test, we use the Shapiro-wilk test for each group of team members. 

```{r shapiro1}
sapply(lapply(sourceDataGroups[1:8], shapiro.test), `[`, c("statistic","p.value"))
qqnorm(sourceDataGroups$V1)
qqline(sourceDataGroups$V1)
qqnorm(sourceDataGroups$V2)
qqline(sourceDataGroups$V2)
qqnorm(sourceDataGroups$V3)
qqline(sourceDataGroups$V3)
qqnorm(sourceDataGroups$V4)
qqline(sourceDataGroups$V4)
qqnorm(sourceDataGroups$V5)
qqline(sourceDataGroups$V5)
qqnorm(sourceDataGroups$V6)
qqline(sourceDataGroups$V6)
qqnorm(sourceDataGroups$V7)
qqline(sourceDataGroups$V7)
qqnorm(sourceDataGroups$V8)
qqline(sourceDataGroups$V8)

```


## Group Size vs. Task Completion Time

We plotted the chart with group size vs. Time and performed the Kruskal test with paired Dunn tests
with Holm-Bonferroni correction:

```{r boxplot_time}
time <- gather(sourceDataGroups, "group", "time", 1:4)
ggplot(time, aes(x=group, y=time)) + geom_boxplot()+labs(x="Group Size", y = "Time (seconds)")+theme_bw()+ scale_x_discrete(breaks=c("V1", "V2", "V3", "V4"),labels=c("1", "2", "3","4"))+ scale_y_continuous(limits = c(0, 300), expand = c(0, 0),breaks = c(0,50,100,150,200,250,300))
```

```{r dunn2 posthoc}
dunn.test(time$time,time$group, kw=TRUE, method="holm")
```

The Kruskal-Wallis test failed to reject equality of medians across different group sizes for task completion time (H(3) = 2.1834, p = 0.54), thus we reject H1 and H3.

## Group Size vs. Task Accuracy

```{r boxplot_acc}
error <- gather(sourceDataGroups, "group", "error", 5:8)
ggplot(error, aes(x=group, y=error)) + geom_boxplot()+labs(x="Group Size", y = "Error")+theme_bw()+ scale_x_discrete(breaks=c("V5", "V6", "V7", "V8"),labels=c("1", "2", "3","4"))+ scale_y_continuous(limits = c(0, 12), expand = c(0, 0),breaks = c(0,2,4,6,8,10,12))
```

```{r dunn1 posthoc}
dunn.test(error$error,error$group, kw=TRUE, method="holm")
```

The Kruskal-Wallis test revealed significant effect of group size on task accuracy median (H(3)= 21.3522, p < 0.0001). The post-hoc Dunn test indicate that
significant accuracy increase occurs between group sizes 1 and 3 (p = 0.0486), 1 and 4 (p < 0.0001), 2 and 4 (p = 0.0024) and 3 and 4 (p = 0.0226). No significant difference exists between group sizes 1 and 2 and between group sizes 2 and 3. This result confirms H4. 


## Groups vs. User Workload
```{r workload}
usersWorkload <- read.csv("workload.csv", header = FALSE,  sep=",")
usersWorkload
```

We extracted the time each participant took to perform each transformation action (translation, rotation, scale and camera) during the task and normalized by the group worked time. Then, we added up all transformations into the final worked time of the participant. We performed an analysis to assess the workload of each team members for all group size.

```{r workload_kruskal}
workload <- gather(usersWorkload, "group", "workload", 1:4)
dunn.test(workload$workload,workload$group, kw=TRUE, method="holm")
```

The Kruskal-Wallis test revealed a significant effect in the workload when group sizes vary (H(3) = 79.0784,p < 0.0001). The Dunn post-hoc indicates significant decrease in workload between groups size 1 and 2 (p < 0.0006), 1 and 3 (p < 0.0001), 1 and 4 (p = 0.0031), 2 and 3 (p < 0.0007), 2 and 4 (p < 0.0001). There was no significance difference effect between group sizes 3 and 4 (p = 0.0303).

## Groups vs. Work Distibution Balance
```{r workdistibution}
workDistribution <- read.csv("work_distribution_balance.csv", header = FALSE,  sep="\t")
workDistribution
```

We computed the work distribution balance. We calculated the group variance using each individual workload. We adjusted the results by multiplying the variance by the group size. In this test, only groups with two, three and four members were evaluated, since the work distribution in groups
with one participant is zero. 

```{r workdistibution_kruskal}
workdistr <- gather(workDistribution, "group", "workdistr", 1:3)
dunn.test(workdistr$workdistr,workdistr$group, kw=TRUE, method="holm")
```

The Kruskal-Wallis test failed to reject equality of medians across different group sizes for work distribution balance (H(2) = 4.628, p = 0.1)


## Correlation between Groups and user role change

```{r roles}
sourceDataRoles <- read.csv("groupsRoles.csv",header = FALSE,  sep="\t") 
sourceDataRoles
sourceGroupsErrorRoles <- read.csv("members_error_roles.csv",header = FALSE,  sep=";")
sourceGroupsErrorRoles
```

To extract the division of tasks between the groups, we identified the frequency users swap between
transformations (translation, rotation, scale). We call this swap a role change.

```{r roles_plot}
ggplot(sourceDataRoles, aes(x=as.character(V1), y=V2)) + geom_boxplot()+labs(x="Group Size", y = "Roles Swap")+theme_bw()+scale_x_discrete(breaks=c("1", "2", "3", "4"),labels=c("1", "2", "3","4"))+ scale_y_continuous(limits = c(0, 48), expand = c(0, 0),breaks = c(0,8,16,24,32,40,48))

dunn.test(sourceDataRoles$V2,sourceDataRoles$V1, kw = TRUE, method="holm")
```
The Kruskal-Wallis variance analysis revealed significant effect between groups and user roles changes (H(3) = 40.1615, p < 0.0001). The post-hoc Dunn test indicates that significant work division occurs between groups size 1 and 3 (p < 0.0001), 1 and 4 (p < 0.0001), 2 and 3 (p = 0.0031) and 2 and 4 (p < 0.0001).

## Accuracy vs. User Roles
Since the accuracy and the work division have similar behaviors, we hypothesize that the two variables were related.

```{r corr_acc_userholes}
sapply(lapply(sourceGroupsErrorRoles[2:3], shapiro.test), `[`, c("statistic","p.value"))

cor.test(sourceGroupsErrorRoles$V2,sourceGroupsErrorRoles$V3, alternative = "greater",method = "spearman")
pairs(sourceGroupsErrorRoles)
lm_out <- lm(V2~V3, data=sourceGroupsErrorRoles)
ggplot(sourceGroupsErrorRoles, aes(x=V3, y=V2)) + geom_point(shape=1) + geom_smooth(method=lm) +labs(title="Accuracy vs. User roles change",x="Role changes", y = "Error")+theme_bw()+ scale_x_continuous(limits = c(0, 50), expand = c(0, 0),breaks = c(0,5,10,15,20,25,30,35,40,45,50))+scale_y_continuous(limits = c(0, 10),expand = c(0, 0),breaks = c(0,2,4,6,8,10))
```

The Pearson correlation revealed a significant effect between accuracy and user roles changes (r = -0.3957,p = <0.0001).


## Analysis of learning between tasks 3 and 4 (Wilcoxon signed rank test)


The hypotheses for the comparison across repeated measures are:

* H0: The distributions (whatever they are) are the same across repeated measures
* H1: The distributions across repeated measures are different

```{r wilcox}
wilcox.test(sourceDataTasks$Time.task.3,sourceDataTasks$Time.task.4,paired=TRUE)
wilcox.test(sourceDataTasks$Error.task.3,sourceDataTasks$Error.task.4,paired=TRUE)

tasksTime <- gather(sourceDataTasks, "task", "time", 2:3)
summary(tasksTime)

ggplot(tasksTime, aes(x=as.character(task), y=time)) + geom_boxplot()+labs(x="Tasks", y = "Time (seconds)")+theme_bw()+scale_x_discrete(breaks=c("Time.task.3", "Time.task.4"),labels=c("First Trial", "Second Trial"))+ scale_y_continuous(limits = c(0, 300), expand = c(0, 0),breaks = c(0,50,100,150,200,250,300))


tasksError <- gather(sourceDataTasks, "task", "error", 4:5)
summary(tasksError)
ggplot(tasksError, aes(x=as.character(task), y=error)) + geom_boxplot()+labs(x="Tasks", y = "Error")+theme_bw()+scale_x_discrete(breaks=c("Error.task.3", "Error.task.4"),labels=c("First Trial", "Second Trial"))+ scale_y_continuous(limits = c(0, 9), expand = c(0, 0),breaks = c(0,1.5,3,4.5,6,7.5,9))
```

